<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Regressão | Introdução ao Machine Learning</title>
  <meta name="description" content="Livro para alunos e alunas que querem iniciar o aprendizado em Machine Learning." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Regressão | Introdução ao Machine Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://dataat.github.io/introducao-ao-machine-learning/" />
  
  <meta property="og:description" content="Livro para alunos e alunas que querem iniciar o aprendizado em Machine Learning." />
  <meta name="github-repo" content="dataat/introducao-ao-machine-learning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Regressão | Introdução ao Machine Learning" />
  
  <meta name="twitter:description" content="Livro para alunos e alunas que querem iniciar o aprendizado em Machine Learning." />
  

<meta name="author" content="Adriano Almeida" />
<meta name="author" content="Felipe Carvalho" />
<meta name="author" content="Felipe Menino" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introdução.html"/>
<link rel="next" href="classificação.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nossos-livros"><i class="fa fa-check"></i>Nossos livros</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licença"><i class="fa fa-check"></i>Licença</a></li>
</ul></li>
<li class="part"><span><b>I Introdução</b></span></li>
<li class="chapter" data-level="1" data-path="introdução.html"><a href="introdução.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="introdução.html"><a href="introdução.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine learning</a></li>
<li class="chapter" data-level="1.2" data-path="introdução.html"><a href="introdução.html#tipos-de-aprendizado"><i class="fa fa-check"></i><b>1.2</b> Tipos de aprendizado</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introdução.html"><a href="introdução.html#aprendizado-supervisionado"><i class="fa fa-check"></i><b>1.2.1</b> Aprendizado supervisionado</a></li>
<li class="chapter" data-level="1.2.2" data-path="introdução.html"><a href="introdução.html#aprendizado-não-supervisionado"><i class="fa fa-check"></i><b>1.2.2</b> Aprendizado Não supervisionado</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Aprendizado Supervisionado</b></span></li>
<li class="chapter" data-level="2" data-path="regressão.html"><a href="regressão.html"><i class="fa fa-check"></i><b>2</b> Regressão</a><ul>
<li class="chapter" data-level="2.1" data-path="regressão.html"><a href="regressão.html#regressão-linear"><i class="fa fa-check"></i><b>2.1</b> Regressão Linear</a><ul>
<li class="chapter" data-level="2.1.1" data-path="regressão.html"><a href="regressão.html#coeficientes-da-regressão-linear"><i class="fa fa-check"></i><b>2.1.1</b> Coeficientes da regressão linear</a></li>
<li class="chapter" data-level="2.1.2" data-path="regressão.html"><a href="regressão.html#exemplos-de-aplicação-do-algoritmo"><i class="fa fa-check"></i><b>2.1.2</b> Exemplos de aplicação do algoritmo</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="regressão.html"><a href="regressão.html#máquinas-de-vetores-de-suporte"><i class="fa fa-check"></i><b>2.2</b> Máquinas de vetores de suporte</a><ul>
<li class="chapter" data-level="2.2.1" data-path="regressão.html"><a href="regressão.html#kernels"><i class="fa fa-check"></i><b>2.2.1</b> <em>Kernels</em></a></li>
<li class="chapter" data-level="2.2.2" data-path="regressão.html"><a href="regressão.html#regressão-com-máquinas-de-vetores-de-suporte"><i class="fa fa-check"></i><b>2.2.2</b> Regressão com máquinas de vetores de suporte</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classificação.html"><a href="classificação.html"><i class="fa fa-check"></i><b>3</b> Classificação</a><ul>
<li class="chapter" data-level="3.1" data-path="classificação.html"><a href="classificação.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>3.1</b> <em>k</em>-Nearest Neighbors</a><ul>
<li class="chapter" data-level="3.1.1" data-path="classificação.html"><a href="classificação.html#como-determinar-o-valor-de-k"><i class="fa fa-check"></i><b>3.1.1</b> Como determinar o valor de K ?</a></li>
<li class="chapter" data-level="3.1.2" data-path="classificação.html"><a href="classificação.html#complexidade"><i class="fa fa-check"></i><b>3.1.2</b> Complexidade</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classificação.html"><a href="classificação.html#árvore-de-decisão"><i class="fa fa-check"></i><b>3.2</b> Árvore de decisão</a><ul>
<li class="chapter" data-level="3.2.1" data-path="classificação.html"><a href="classificação.html#conceitos-gerais"><i class="fa fa-check"></i><b>3.2.1</b> Conceitos gerais</a></li>
<li class="chapter" data-level="3.2.2" data-path="classificação.html"><a href="classificação.html#funcionamento"><i class="fa fa-check"></i><b>3.2.2</b> Funcionamento</a></li>
<li class="chapter" data-level="3.2.3" data-path="classificação.html"><a href="classificação.html#problemas-com-overfitting"><i class="fa fa-check"></i><b>3.2.3</b> Problemas com overfitting</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Aprendizado Não Supervisionado</b></span></li>
<li class="chapter" data-level="4" data-path="agrupamento.html"><a href="agrupamento.html"><i class="fa fa-check"></i><b>4</b> Agrupamento</a><ul>
<li class="chapter" data-level="4.1" data-path="agrupamento.html"><a href="agrupamento.html#o-que-é-um-agrupamento"><i class="fa fa-check"></i><b>4.1</b> O que é um agrupamento?</a></li>
<li class="chapter" data-level="4.2" data-path="agrupamento.html"><a href="agrupamento.html#técnicas-de-agrupamento"><i class="fa fa-check"></i><b>4.2</b> Técnicas de agrupamento</a><ul>
<li class="chapter" data-level="4.2.1" data-path="agrupamento.html"><a href="agrupamento.html#técnicas-baseadas-em-partição"><i class="fa fa-check"></i><b>4.2.1</b> Técnicas baseadas em Partição</a></li>
<li class="chapter" data-level="4.2.2" data-path="agrupamento.html"><a href="agrupamento.html#técnicas-baseadas-em-hierarquia"><i class="fa fa-check"></i><b>4.2.2</b> Técnicas baseadas em Hierarquia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="agrupamento.html"><a href="agrupamento.html#kmeans"><i class="fa fa-check"></i><b>4.3</b> Kmeans</a><ul>
<li class="chapter" data-level="4.3.1" data-path="agrupamento.html"><a href="agrupamento.html#como-avaliar-o-kmeans"><i class="fa fa-check"></i><b>4.3.1</b> Como avaliar o Kmeans?</a></li>
<li class="chapter" data-level="4.3.2" data-path="agrupamento.html"><a href="agrupamento.html#como-definir-a-quantidade-de-grupos"><i class="fa fa-check"></i><b>4.3.2</b> Como definir a quantidade de grupos?</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="agrupamento.html"><a href="agrupamento.html#agrupamento-hierarquico---método-aglomerativo"><i class="fa fa-check"></i><b>4.4</b> Agrupamento Hierarquico - Método Aglomerativo</a><ul>
<li class="chapter" data-level="4.4.1" data-path="agrupamento.html"><a href="agrupamento.html#qual-método-de-ligação-deve-ser-usado"><i class="fa fa-check"></i><b>4.4.1</b> Qual método de ligação deve ser usado?</a></li>
<li class="chapter" data-level="4.4.2" data-path="agrupamento.html"><a href="agrupamento.html#como-visualizar-os-grupos-no-dendrograma"><i class="fa fa-check"></i><b>4.4.2</b> Como visualizar os grupos no dendrograma?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="exemplos.html"><a href="exemplos.html"><i class="fa fa-check"></i><b>5</b> Exemplos</a></li>
<li class="chapter" data-level="6" data-path="considerações-finais.html"><a href="considerações-finais.html"><i class="fa fa-check"></i><b>6</b> Considerações finais</a></li>
<li class="part"><span><b>IV Apendice</b></span></li>
<li class="chapter" data-level="7" data-path="sobre-o-kaggle.html"><a href="sobre-o-kaggle.html"><i class="fa fa-check"></i><b>7</b> Sobre o Kaggle</a><ul>
<li class="chapter" data-level="7.1" data-path="sobre-o-kaggle.html"><a href="sobre-o-kaggle.html#cadastro"><i class="fa fa-check"></i><b>7.1</b> Cadastro</a></li>
<li class="chapter" data-level="7.2" data-path="sobre-o-kaggle.html"><a href="sobre-o-kaggle.html#criação-de-um-notebook"><i class="fa fa-check"></i><b>7.2</b> Criação de um notebook</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="referências-bibliográficas.html"><a href="referências-bibliográficas.html"><i class="fa fa-check"></i><b>8</b> Referências Bibliográficas</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introdução ao Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regressão" class="section level1">
<h1><span class="header-section-number">2</span> Regressão</h1>
<p>Todas as pessoas pelo menos uma vez na vida já sentiu ou sentirá a necessidade
de prever algum acontecimento futuro. Estamos a todo momento assimilando
informações para realizar alguma tomada de decisão, seja de forma intrínseca ou
não. No contexto de Machine Learning (ML) isso é feito pela técnicas de
regressão. A regressão é uma ferramenta que busca modelar relações entre
variáveis dependentes e independentes através de métodos estatísticos
<span class="citation">(Soto <a href="#ref-soto2013regression">2013</a>)</span>.</p>
<p>Uma variável independente, normalmente representada pela variável <span class="math inline">\(x\)</span>,
caracteriza uma grandeza que está sendo manipulada durante um experimento e que
não sofre influência de outras variáveis. Já a variável dependente, normalmente
representada pela variável <span class="math inline">\(y\)</span>, caracteriza valores que estão diretamente
associados à variável independente, ou seja, ao ser manipulada os valores
variável independente, o valor das variáveis dependentes também sofrem
alterações. Na Figura <a href="regressão.html#fig:happinessWorld">2.1</a> é apresentada a relação entre a
expectativa de vida baseada e um índice de felicidade calculado em diversos
países obtidos a partir de um levantamento feito por <span class="citation">Helliwell et al. (<a href="#ref-helliwell2020social">2020</a>)</span>. A
variável independente nesse exemplo é representada pelo índice de felicidade e a
expectativa de vida age como variável independente, dessa forma pode ser
observada uma tendência de expectativa de vida maior em países com alto índice
de felicidade, com uma força de correlação de 0,77.</p>
<div class="figure" style="text-align: center"><span id="fig:happinessWorld"></span>
<img src="assets/03_regression/happiness_world.png" alt="Relação entre o índice de felicidade e expectativa de vida. Fonte: [@helliwell2020social]" width="1847" />
<p class="caption">
Figure 2.1: Relação entre o índice de felicidade e expectativa de vida. Fonte: <span class="citation">(Helliwell et al. <a href="#ref-helliwell2020social">2020</a>)</span>
</p>
</div>
<p>As relações entre as variáveis dependentes e independetes são feitas através de
algum coeficiente de correlação. Uma das métricas de correlação mais utilizadas
é o coeficiente de Pearson, que mede a associação linear entre duas variáveis
<span class="citation">(Kirch <a href="#ref-kirck2008pearson">2008</a>)</span>. Esse coeficiente de correlação pode ser definido pela
Equação <a href="regressão.html#eq:corr-pearson">(2.1)</a>, onde <span class="math inline">\(n\)</span> é o total de amostras, <span class="math inline">\(\overline{x}\)</span>
e <span class="math inline">\(\overline{y}\)</span> são as médias aritméticas de ambas as variáveis. Os valores do
coeficiente de Pearson variam entre -1 e 1, de tal forma que quanto mais
próximos desses extremos, melhor correlacionado estão as variáveis. A Figura
<a href="regressão.html#fig:scatterCorrelations">2.2</a> mostra alguns exemplos com gráficos de disperssão
de variáveis com diferentes correlações.</p>
<p><span class="math display" id="eq:corr-pearson">\[\begin{equation} 
    r_{xy} = \frac{{}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}
    {\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}}
\tag{2.1}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:scatterCorrelations"></span>
<img src="assets/03_regression/correlations.png" alt="Diferentes correlações entre variáveis. Fonte: [@helliwell2020social]" width="2014" />
<p class="caption">
Figure 2.2: Diferentes correlações entre variáveis. Fonte: <span class="citation">(Helliwell et al. <a href="#ref-helliwell2020social">2020</a>)</span>
</p>
</div>
<p>Os métodos de regressão se utilizam dessas correlações entre as variáveis para
estimar valores não existentes na amostra ou conjunto de dados. Entretanto, nem
sempre essas correlações são tão explicítas assim, sendo necessário outras
abordagens mais robustas para realizar as previsões.Em ML os modelos de
regressão podem ser criados a partir de diversas abordagens, desde as mais
simples com poucas configurações de parâmetros e de fácil interpretação do
funcionamento, até as abordagens mais complexas. Os métodos de regressão
abordados neste capítulo serão <code>Regressão linear</code>,
<code>Máquina de vetores de suporte</code> e <code>Árvores de decisão</code>.</p>
<div id="regressão-linear" class="section level2">
<h2><span class="header-section-number">2.1</span> Regressão Linear</h2>
<p><a href="https://www.kaggle.com/lordadriano/mc2-worcap-2020-linear-regression"><img src="https://suspicious-wescoff-e06084.netlify.app/badge-perguntas.svg" alt="Questao disponivel" /></a></p>
<p>A regressão linear é um dos métodos mais intuitivos e utilizados para essa
finalidade. Esses métodos são dividos em dois grupos, a regressão linear simples
(RLS) e regressão linear múltipla (RLM). A RLS tem como objetivo estabelecer uma
relação entre duas variáveis através de uma função, que pode ser definida por:</p>
<p><span class="math display" id="eq:rls-function">\[\begin{equation} 
    y_{i} = \alpha+\beta x_{i}
\tag{2.2}
\end{equation}\]</span></p>
<p>Onde <span class="math inline">\(y_{i}\)</span> é a variável alvo, <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(beta x_{i}\)</span> são coeficientes
calculados pela regressão, que representam o intercepto no eixo <span class="math inline">\(y\)</span> e inclinação
da reta, respectivamente.</p>
<p>A RLM é semelhante semelhante à RLS, porém possui multiplas variáveis
preditoras, e pode ser definida por:</p>
<p><span class="math display" id="eq:rlm-function">\[\begin{equation} 
    y_{i} = \alpha+\beta x_{i1}+\beta x_{i2}+...+\beta x_{in}
\tag{2.3}
\end{equation}\]</span></p>
<p>Onde <span class="math inline">\(y_{i}\)</span> é a variável alvo, <span class="math inline">\(\alpha\)</span> continua sendo o coeficiente de
intercepto e <span class="math inline">\(\beta x_{ip}\)</span> o é coeficiente angular da <span class="math inline">\(p\)</span>-ésima variável. Ambos
os métodos podem ainda serem somados a um termo <span class="math inline">\(\epsilon\)</span> de erro.</p>
<div id="coeficientes-da-regressão-linear" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Coeficientes da regressão linear</h3>
<p>Existem diversas abordagens para se calcular os coeficientes <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>
da equação da regressão linear, as técnicas baseadas em mínimos quadrados
oridinários e gradiente descendente são as mais comuns. A seguir serão
apresentados os funcionamentos dessas abordagens.</p>
<div id="métodos-dos-quadrados-ordinários" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Métodos dos quadrados ordinários</h4>
<p>O Método dos quadrados ordinários (MQO) ou método dos mínimos quadrados (MMQ),
busca encontrar o melhor valor para os coeficientes citados anteriormente, de
tal forma que a diferença absoluta entre o valor real e o predito pela função
seja a menor possível entre todos os pontos. A Figura <a href="regressão.html#fig:ols">2.3</a> mostra um
exmplo de regressão linar utilizando o MQO para o conjunto pontos descritos na
tabela a seguir:</p>
<p>| Variável independente | Variável dependente |
| 0.44 | 5.52 |
| 1.74 | 8.89 |
| 0.41 | 4.05 |
| 1.84 | 9.31 |
| 0.98 | 6.57 |
| 1.22 | 8.27 |
| 1.53 | 6.93 |
| 1.04 | 6.41 |
| 0.59 | 6.93 |
| 0.38 | 6.98 |</p>
<div class="figure" style="text-align: center"><span id="fig:ols"></span>
<img src="assets/03_regression/ols.png" alt="Exemplo do método dos quadrados ordinários." width="1220" />
<p class="caption">
Figure 2.3: Exemplo do método dos quadrados ordinários.
</p>
</div>
<p>Para se chegar no resultado apresentado na Figura <a href="regressão.html#fig:ols">2.3</a>, os
coeficientes da regressão linear foram ajustados utilizando derivadas parciais,
de tal tal forma que o erro quadrático médio entre entre a função e cada um dos
pontos fossem a menor possível. A Figura <a href="regressão.html#fig:ols-steps">2.4</a> mostra o ajuste
dos coeficientes da equação em relação a cada ponto.</p>
<div class="figure" style="text-align: center"><span id="fig:ols-steps"></span>
<img src="assets/03_regression/ols-steps.png" alt="Ajuste da regressão linear por método dos quadrados ordinários." width="2390" />
<p class="caption">
Figure 2.4: Ajuste da regressão linear por método dos quadrados ordinários.
</p>
</div>
</div>
<div id="gradiente-descendente" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> Gradiente descendente</h4>
<p>O método do gradiente descendente (GD) é uma das técnicas mais utilizadas para
otimização de modelos de ML. Este é um método interativo que busca encontrar os
coeficiente <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> através da minimização de uma função de custo,
que normalmente é o erro quadrático médio (MSE - sigla do inglês,
<em>mean squared error</em>).</p>
<p>O GD funciona de forma iterativa e inicializa os coeficientes com um valor
predefinido ou aleatório. Em cada iteração é obtido o somatório do erro entre
todos os valores das variáveis dependentes e valores calculados pela função. Com
base nesse erro e em uma taxa de aprendizagem do modelo predefinida, os valores
dos coeficientes da função são atualizados para a próxima iteração. A taxa de
aprendizagem deve ser definda com um valor equilibrado. A definição de um valor
muito alto para a taxa de aprendizagem pode levar o modelo a cair em um mínimo
local, ou seja, o modelo não consegue chegar em seu melhor ajuste. Já quando a
taxa de aprendizagem é definida com um valor muito baixo, o modelo demora mais
tempo para chegar no ajuste ideal, necessitando de muito mais tempo e
processamento até que haja a convergência. A Figura <a href="regressão.html#fig:learning-rate">2.5</a>
mostra o comportamento do GD com diferentes categorias de valores mencionadas
para a taxa de aprendizagem.</p>
<div class="figure" style="text-align: center"><span id="fig:learning-rate"></span>
<img src="assets/03_regression/learning-rate-gd.png" alt="Problemas na taxa de aprendizado do gradiente descendente." width="3410" />
<p class="caption">
Figure 2.5: Problemas na taxa de aprendizado do gradiente descendente.
</p>
</div>
<p>Os prinicipais parâmetros a serem definidos nessa abordagem são a taxa de
aprendizagem e o número de iterações. Considerando os pontos utilizados no
exemplo anterior, foi aplicada a regressão linear utilizando o GD como método de
atualização dos coeficientes. A Figura <a href="regressão.html#fig:gd-example1">2.6</a> mostra o ajuste da
função, custo e os coeficientes <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> ao longo de 50 iterações com
taxa de aprendizado muito baixa. Nessa figura pode ser observado que as
iterações finalizam antes da convergência do modelo.</p>
<div class="figure" style="text-align: center"><span id="fig:gd-example1"></span>
<img src="assets/03_regression/gradient-descendent-small.png" alt="Regressão linear com taxa de aprendizagem baixa no gradiente descendente." width="2440" />
<p class="caption">
Figure 2.6: Regressão linear com taxa de aprendizagem baixa no gradiente descendente.
</p>
</div>
<p>Como mencionado anteriormente, uma taxa de aprendizagem muito grande também
interfere no ajuste dos coeficientes, uma vez o modelo não consegue atingir o
mínimo global. A Figura <a href="regressão.html#fig:gd-example2">2.7</a> mostra o resultado da execução da
regressão linear utilizando uma taxa de aprendizagem muito grande no GD.</p>
<div class="figure" style="text-align: center"><span id="fig:gd-example2"></span>
<img src="assets/03_regression/gradient-descendent-large.png" alt="Regressão linear com taxa de aprendizagem alta no gradiente descendente." width="2422" />
<p class="caption">
Figure 2.7: Regressão linear com taxa de aprendizagem alta no gradiente descendente.
</p>
</div>
<p>Já com uma taxa de aprendizagem equilibrada, o GD é capaz de ajustar os
coeficientes de forma mais eficiente. A Figura <a href="regressão.html#fig:gd-example3">2.8</a> mostra
o resultado do algoritmo executado com uma taxa de aprendizagem mais
equilibrada. Como os valores iniciais dos coeficientes são definidos de forma
aleatória, nas primeiras iterações o gradiente apresenta uma alta pertubação,
que vai se atenuando ao longo das épocas.</p>
<div class="figure" style="text-align: center"><span id="fig:gd-example3"></span>
<img src="assets/03_regression/gradient-descendent.png" alt="Regressão linear com taxa de aprendizagem equilibrada no gradiente descendente." width="2422" />
<p class="caption">
Figure 2.8: Regressão linear com taxa de aprendizagem equilibrada no gradiente descendente.
</p>
</div>
<p>Para dados com poucas dimensões, ou seja, poucas variáveis preditoras, o MQO é
mais recomendado, pois diferente do GD, não é um algoritmo interativo, e sua
complexidade está associada diretamente à quantidade de pontos. Já o GD tem
melhor performance quando os dados possuem muitas dimensões.</p>
<p>A regressão linear pode ser aplicada em uma vasta variedade de problemas, mas
como foi apresentado ao longo desta seção, é necessário que os dados possuam uma
alta correlação. Este algoritmo está disponível na biblioteca <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">Scikit-learn</a>
par ser utilizado em Python.</p>
</div>
</div>
<div id="exemplos-de-aplicação-do-algoritmo" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Exemplos de aplicação do algoritmo</h3>
<p>Para você colocar me prática os conceitos vistos, abaixo há um exemplo de aplicação do algoritmo.</p>
<ul>
<li><a href="https://www.kaggle.com/lordadriano/mc2-worcap-2020-linear-regression">Exemplo de Regressão Linear</a></li>
</ul>
</div>
</div>
<div id="máquinas-de-vetores-de-suporte" class="section level2">
<h2><span class="header-section-number">2.2</span> Máquinas de vetores de suporte</h2>
<p><a href="https://www.kaggle.com/lordadriano/mc2-worcap-2020-svr"><img src="https://suspicious-wescoff-e06084.netlify.app/badge-perguntas.svg" alt="Questao disponivel" /></a></p>
<p>As máquinas de vetores de suporte (SVM - sigla do inglês,
<em>support vector machine</em>) são modelos de aprendizado de máquina supervisionado
concebido a partir de um conceito inicialmente proposto por <span class="citation">Vapnik and Chervonenkis (<a href="#ref-vapnik1965class">1963</a>)</span>.
As SVM podem ser utilizadas tanto para tarefas de classificação, quanto para
tarefas de regressão, sendo uma ótima alteranativa aos modelos de redes neurais
artificiais profundas que exigem um custo computacional muito superior em dados
com muitas dimensões. Outra vantagem na utilização dos modelos baseados em SVM é
que eles não são sensíveis aos <em>outliers</em>, ou seja, valores extremos não causam
ruído no treinamento.</p>
<p>O funcionamento básico das SVM consiste em ajustar a equação de uma reta,
denominada hiperplano de tal forma que a distância entre ela e os pontos com
características diferentes seja maximizada. Um conjunto de <span class="math inline">\(n\)</span> pontos é definido
como <span class="math inline">\((\vec{x_{1}}, y_{1}), (\vec{x_{2}}, y_{2}), ..., (\vec{x_{n}}, y_{n})\)</span>,
onde <span class="math inline">\(\vec{x_{i}}\)</span> são as variáveis independentes representadas por um vetor de
<span class="math inline">\(d\)</span>-dimensões e <span class="math inline">\(y_{i}\)</span> são as variáveis dependentes. A distância maximizada
entre o hiperplano e as fronteiras são definidas como margens e os pontos que
estão no limite dessa margem são os vetores de suporte. Esses componentes podem
ser modelados da seguinte forma:</p>
<p><span class="math display" id="eq:svm-components">\[\begin{equation} 
    \vec{w}\cdot\vec{x}-b =
      \begin{cases}
        &amp; -1, &amp; \text{primeiro vetor de suporte} \\
        &amp; 0, &amp; \text{hiperplano} \\
        &amp; 1, &amp; \text{segundo vetor de suporte}
      \end{cases}
\tag{2.4}
\end{equation}\]</span></p>
<p>Onde <span class="math inline">\(\vec{w}\)</span> é um vetor perpendicular aos pontos, <span class="math inline">\(\vec{x}\)</span> é o vetor do
conjunto de pontos é <span class="math inline">\(b\)</span> é uma constante opcional que pode ser usada como uma
<em>bias</em>. Quando o resultado dessa equação é igual a <span class="math inline">\(1\)</span> ou <span class="math inline">\(-1\)</span> trata-se de um
dos vetores de suporte, quando o resultado é um valor maior que <span class="math inline">\(0\)</span> e menor que
<span class="math inline">\(1\)</span> ou menor que <span class="math inline">\(0\)</span> e maior que <span class="math inline">\(-1\)</span> trata-se de um espaço da margem. A Figura
<a href="regressão.html#fig:linear-svm">2.9</a> mostra um exemplo da aplicação do algoritmo SVM em um
conjunto de dados linearmente separáveis. Nessa figura, o hiperplano é
caracterizado pela linha contínua, os vetores de suporte são as linhas
tracejadas que interceptam os pontos com contorno destacado, e o espaço entre
eles são as margens.</p>
<div class="figure" style="text-align: center"><span id="fig:linear-svm"></span>
<img src="assets/03_regression/linear-svm.png" alt="SVM para conjunto de dados linearmente separáveis." width="995" />
<p class="caption">
Figure 2.9: SVM para conjunto de dados linearmente separáveis.
</p>
</div>
<p>As primeiras versões das SVM eram limitadas somente para resolução de problemas
linearmente separáveis, como mostrado no exemplo anterior, mas a grande maioria
dos problemas não são linearmente separáveis. Considerando a Figura
<a href="regressão.html#fig:kernels-problem">2.10</a> é muito difícil traçar um hiperplano que separe bem
os pontos de cores diferentes. Uma alternativa para esse problema é aumentar as
dimensões para a representação do hiperplano. Essa tarefa é feita com a
introdução de um conceito definido <em>kernel</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:kernels-problem"></span>
<img src="assets/03_regression/kernels-problem.png" alt="Conjunto de dados não linearmente separáveis." width="910" />
<p class="caption">
Figure 2.10: Conjunto de dados não linearmente separáveis.
</p>
</div>
<p>Ao traçar um hiperplano não linear com a utilização de <em>kernels</em> é possível
ajustar melhor os vetores de suporte aos dados. A Figura <a href="regressão.html#kernels">2.2.1</a> mostra o
conjunto de dados ajustado com hiperplanos lineares e não lineares.</p>
<div class="figure" style="text-align: center"><span id="fig:kernels"></span>
<img src="assets/03_regression/kernels.png" alt="Hiperplanos utilizando *kernels* com funções lineares e não lineares." width="1925" />
<p class="caption">
Figure 2.11: Hiperplanos utilizando <em>kernels</em> com funções lineares e não lineares.
</p>
</div>
<p>A abordagem utilizando os <em>kernels</em> é uma das principais características desse
modelo de ML, pois faz com que o hiperplano seja ajustado em uma dimensão
superior, utilizando equações de polinômios de maior grau. A Figura
<a href="regressão.html#fig:kernels-plot">2.12</a> mostra graficamente como é realizada essa manipulação.</p>
<div class="figure" style="text-align: center"><span id="fig:kernels-plot"></span>
<img src="assets/03_regression/kernels-plot.png" alt="Representação gráfica dos dados e da função não linear." width="1816" />
<p class="caption">
Figure 2.12: Representação gráfica dos dados e da função não linear.
</p>
</div>
<p>A utilização de <em>kernels</em> é uma das principais características do SVM e faz com
que os modelos baseados nessa abordagem, sejam tão robustos quanto outras
técnicas mais complexas.</p>
<div id="kernels" class="section level3">
<h3><span class="header-section-number">2.2.1</span> <em>Kernels</em></h3>
<p>A utilização dos <em>kernels</em> em SVM foram introduzidos por <span class="citation">Boser, Guyon, and Vapnik (<a href="#ref-boser1992training">1992</a>)</span>.
Esses componentes <em>kernels</em> são responsáveis por criar uma transformação dos
dados a partir de uma função, que são responsáveis por maximizar as margens dos
vetores de suporte. A maioria das bibliotecas de ML, já possuem <em>kernels</em>
implementados e também permitem a integração de outras funções customizadas. A
lista a seguir aprensenta brevemente algunso dos principais <em>kernels</em>
utilizados.</p>
<ul>
<li><strong>Linear: </strong> Como mencionado anteriormente, é eficiente somente para problemas
linearmente separáveis, uma vez que seu ajuste se da através da equação de uma
reta. O <em>kernel</em> linear é definido apenas pelo produto entre duas amostras
<span class="math inline">\(\vec{x_{i}}\)</span> e <span class="math inline">\(\vec{x_{j}}\)</span>:</li>
</ul>
<p><span class="math display" id="eq:linear-kernel">\[\begin{equation} 
    k(\vec{x_{i}}, \vec{x_{j}}) = \vec{x_{i}} \cdot \vec{x_{j}}
\tag{2.5}
\end{equation}\]</span></p>
<ul>
<li><strong>Polinominal: </strong> Os <em>kernels</em> polinomiais popularmente utilizados em tarefas
de processamento de imagens, permitem adicionar curvas aos hiperplanos. Além das
amostras <span class="math inline">\(\vec{x_{i}}\)</span> e <span class="math inline">\(\vec{x_{j}}\)</span>, o <em>kernel</em> polinominal também recebe o
a variável <span class="math inline">\(d\)</span> que indica o seu grau, como definido pela equação:</li>
</ul>
<p><span class="math display" id="eq:polynomial-kernel">\[\begin{equation} 
    k(\vec{x_{i}}, \vec{x_{j}}) = (\vec{x_{i}} \cdot \vec{x_{j}} + 1)^{d}
\tag{2.6}
\end{equation}\]</span></p>
<ul>
<li><strong>Função gaussiana de base radial: </strong> Os <em>kernels</em> RBF
(<em>radial basis function</em>), como também são chamados, são recomendados quando não
se tem um conhecimento prévio a cerca dos dados. Esse <em>kernel</em> realiza uma
transformação dos pontos utilizando uma função gaussiana, definida por:</li>
</ul>
<p><span class="math display" id="eq:rbf-kernel">\[\begin{equation} 
    k(\vec{x_{i}}, \vec{x_{j}}) = exp \left(-\frac{\lVert\vec{x_{i}} - \vec{x_{j}}\rVert^2}{2\sigma^2} \right)
\tag{2.7}
\end{equation}\]</span></p>
</div>
<div id="regressão-com-máquinas-de-vetores-de-suporte" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Regressão com máquinas de vetores de suporte</h3>
<p>Embora as SVM sejam aplicadas principalmente para tarefas de classificação, ou
seja, um cálculo de um valor inteiro, ela também pode ser utilizada para tarefas
de regressão, calculando valores reais. Essas abordagens são chamadas de
regressão por vetores de suporte (SVR - sigla do inglês,
<em>support vector regression</em>) e foram propostas por <span class="citation">Drucker et al. (<a href="#ref-drucker1997support">1997</a>)</span>.
Diferente de modelos tradicionais como as técnicas de regressão apresentadas na
seção anterior, que utilizam derivadas para os cálculos dos intervalos de
confiança, no SVR os valores são previstos através dos hiperplanos.</p>
<p>Este método utiliza para a regressão a abordagem de classificação apresentada
anteriormente, porém com uma pequena variação na função objetivo, que agora
busca comportar dentro das margens comportar a maior quantidade de pontos. As
margens (<span class="math inline">\(\epsilon\)</span>) representam os intervalos de confiança e os vetores de
suporte que as delimitam, representam os limites para os erros positivos (<span class="math inline">\(\xi\)</span>)
e negativos (<span class="math inline">\(\xi^{\ast}\)</span>). A Figura <a href="regressão.html#fig:svr">2.13</a> mostra uma representação de
um hiperplano não linear traçado para a regressão de dados.</p>
<div class="figure" style="text-align: center"><span id="fig:svr"></span>
<img src="assets/03_regression/svr.png" alt="Representação de hiperplano não linear para regressão. Adaptado de @drucker1997support." width="992" />
<p class="caption">
Figure 2.13: Representação de hiperplano não linear para regressão. Adaptado de <span class="citation">Drucker et al. (<a href="#ref-drucker1997support">1997</a>)</span>.
</p>
</div>

</div>
</div>
</div>
<h3> Referências Bibliográficas</h3>
<div id="refs" class="references">
<div id="ref-boser1992training">
<p>Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. “A Training Algorithm for Optimal Margin Classifiers.” In <em>Proceedings of the Fifth Annual Workshop on Computational Learning Theory</em>, 144–52.</p>
</div>
<div id="ref-drucker1997support">
<p>Drucker, Harris, Christopher JC Burges, Linda Kaufman, Alex J Smola, and Vladimir Vapnik. 1997. “Support Vector Regression Machines.” In <em>Advances in Neural Information Processing Systems</em>, 155–61.</p>
</div>
<div id="ref-helliwell2020social">
<p>Helliwell, John F, Haifang Huang, Shun Wang, and Max Norton. 2020. “Social Environments for World Happiness.” <em>World Happiness Report 2020</em>.</p>
</div>
<div id="ref-kirck2008pearson">
<p>Kirch, Wilhelm, ed. 2008. “Pearson’s Correlation Coefficient.” In <em>Encyclopedia of Public Health</em>, 1090–1. Dordrecht: Springer Netherlands. <a href="https://doi.org/10.1007/978-1-4020-5614-7_2569" class="uri">https://doi.org/10.1007/978-1-4020-5614-7_2569</a>.</p>
</div>
<div id="ref-soto2013regression">
<p>Soto, Timothy. 2013. “Regression Analysis.” In <em>Encyclopedia of Autism Spectrum Disorders</em>, edited by Fred R. Volkmar, 2538–8. New York, NY: Springer New York.</p>
</div>
<div id="ref-vapnik1965class">
<p>Vapnik, Vladimir N, and Alexey Y Chervonenkis. 1963. “On a Class of Pattern-Recognition Learning Algorithms.” <em>Automation and Remote Control</em> 25 (6). PLENUM PUBL CORP CONSULTANTS BUREAU, 233 SPRING ST, NEW YORK, NY 10013: 838.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introdução.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classificação.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
